{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Load EmoLex\n",
    "Import all necessary libraries and load the NRC Emotion Lexicon for emotion analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sedra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sedra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "emolex_path = 'NRC_emotion_lexicon_list.txt'\n",
    "emolex = pd.read_csv(emolex_path, sep='\\t', names=['word', 'emotion', 'association'])\n",
    "emolex = emolex[emolex['association'] == 1]\n",
    "emolex_pivot = emolex.pivot(index='word', columns='emotion', values='association').fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define General Functions\n",
    "This section contains reusable functions for text preprocessing, tokenization, removing stop words, lemmatization, and emotion analysis.\n",
    "\n",
    "### Included Functions:\n",
    "1. `clean_text`: Cleans text by removing diacritics, punctuation, and converting to lowercase.\n",
    "2. `basic_tokenize_text`: Splits text into tokens (words).\n",
    "3. `remove_stop_words`: Removes common English stop words.\n",
    "4. `lemmatize_tokens`: Reduces tokens to their base form using lemmatization.\n",
    "5. `calculate_emotions`: Maps lemmatized tokens to emotions using the EmoLex lexicon.\n",
    "6. `generate_ngrams`: Generates n-grams (e.g., bigrams, trigrams) from tokens.\n",
    "7. `unify_columns`: Unifies column names, optionally capitalizes character names, and adds a series identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    text = unidecode(text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) \n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def basic_tokenize_text(text):\n",
    "    return str(text).lower().split()\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stop_words(tokens):\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "\n",
    "def calculate_emotions(lemmas, lexicon):\n",
    "    emotions = lexicon.reindex(lemmas).sum()\n",
    "    return emotions\n",
    "\n",
    "\n",
    "def generate_ngrams(tokens, n):\n",
    "    return list(zip(*[tokens[i:] for i in range(n)]))\n",
    "\n",
    "\n",
    "def unify_columns(data, series_name, character_column, line_column, capitalize_characters=False):\n",
    "    data = data.rename(columns={character_column: 'character', line_column: 'line'})\n",
    "    \n",
    "    if capitalize_characters:\n",
    "        data['character'] = data['character'].str.title()\n",
    "    \n",
    "    data['series'] = series_name\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Analysis Functions\n",
    "Functions for specific phrases and n-gram analysis.\n",
    "1. `analyze_specific_phrases` : Analyzes specific phrases and their occurences across characters.\n",
    "2. `analyze_ngrams` : Analyzes unigrams, bigrams and trigrams across characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_specific_phrases(data, series_name, character_column, phrases_to_search):\n",
    "\n",
    "    all_characters = data[character_column].unique()\n",
    "    results = []\n",
    "\n",
    "    for phrase in phrases_to_search:\n",
    "        filtered = data[data['line'].str.contains(phrase, case=False, na=False)]\n",
    "        counts = filtered[character_column].value_counts()\n",
    "\n",
    "        counts_full = {char: counts.get(char, 0) for char in all_characters}\n",
    "\n",
    "        results.append({\n",
    "            'phrase': phrase,\n",
    "            'series': series_name,\n",
    "            'total': counts.sum(),\n",
    "            **counts_full\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(f'FINAL_{series_name}_specific_phrases.csv', index=False)\n",
    "\n",
    "def analyze_ngrams(data, series_name, character_column):\n",
    "\n",
    "    all_characters = data[character_column].unique()\n",
    "    ngram_results = []\n",
    "\n",
    "    for n in [1, 2, 3]: \n",
    "        data[f'{n}_grams'] = data['lemmas'].apply(lambda x: list(generate_ngrams(x, n)))\n",
    "\n",
    "        all_ngrams = list(itertools.chain.from_iterable(data[f'{n}_grams']))\n",
    "        all_ngrams = [ngram for ngram in all_ngrams if 'im' not in ngram]\n",
    "\n",
    "        ngram_counts = Counter(all_ngrams)\n",
    "\n",
    "        top_ngrams = ngram_counts.most_common(10)\n",
    "\n",
    "        for ngram, count in top_ngrams:\n",
    "            character_counts = data[data[f'{n}_grams'].apply(lambda x: ngram in x)][character_column].value_counts()\n",
    "\n",
    "            character_counts_full = {char: character_counts.get(char, 0) for char in all_characters}\n",
    "\n",
    "            ngram_results.append({\n",
    "                'ngram': ' '.join(ngram),\n",
    "                'ngram_type': f'{n}-gram',\n",
    "                'series': series_name,\n",
    "                'count': count,\n",
    "                **character_counts_full\n",
    "            })\n",
    "\n",
    "    ngram_results_df = pd.DataFrame(ngram_results)\n",
    "    ngram_results_df.to_csv(f'FINAL_{series_name}_ngrams.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Main Processing Function\n",
    "This function combines all steps: text cleaning, emotion analysis, and phrase/n-gram analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(data, series_name, character_column, phrases_to_search):\n",
    "\n",
    "    data['line'] = data['line'].fillna('').astype(str)\n",
    "    data['clean_line'] = data['line'].apply(clean_text)\n",
    "    data['tokens'] = data['clean_line'].apply(basic_tokenize_text)\n",
    "    data['tokens'] = data['tokens'].apply(remove_stop_words)\n",
    "    data['lemmas'] = data['tokens'].apply(lemmatize_tokens)\n",
    "    data['gender'] = data['character'].map(gender_mapping[series_name]).fillna('Unknown')\n",
    "\n",
    "    emotion_columns = emolex_pivot.columns\n",
    "    data[emotion_columns] = data['lemmas'].apply(lambda lemmas: calculate_emotions(lemmas, emolex_pivot))\n",
    "\n",
    "    data.to_csv(f'FINAL_{series_name}_data_with_emotions.csv', index=False)\n",
    "\n",
    "    analyze_specific_phrases(data, series_name, character_column, phrases_to_search)\n",
    "\n",
    "    analyze_ngrams(data, series_name, character_column)\n",
    "\n",
    "    ngram_columns = [col for col in data.columns if '_grams' in col]\n",
    "    data = data.drop(columns=ngram_columns, errors='ignore')\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets\n",
    "In this section, we load the raw data for the analysis.\n",
    "Each dataset will be inspected to verify that all required columns are present before proceeding to data processing and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIMYM dataset loaded with columns: ['season', 'episode_num', 'name', 'line', 'title', 'original_air_date', 'imdb_rating', 'total_votes', 'desc', 'word_count']\n",
      "TBBT dataset loaded with columns: ['season', 'episode_num', 'dialogue', 'person_scene', 'title', 'original_air_date', 'imdb_rating', 'total_votes', 'desc', 'word_count']\n",
      "Friends dataset loaded with columns: ['season', 'episode_num', 'character', 'line', 'title', 'original_air_date', 'imdb_rating', 'total_votes', 'desc', 'word_count']\n"
     ]
    }
   ],
   "source": [
    "himym_path = 'FINAL_himym_clean.csv'\n",
    "tbbt_path = 'FINAL_tbbt_clean.csv'\n",
    "friends_path = 'FINAL_friends_clean.csv'\n",
    "\n",
    "himym_data = pd.read_csv(himym_path)\n",
    "tbbt_data = pd.read_csv(tbbt_path)\n",
    "friends_data = pd.read_csv(friends_path)\n",
    "\n",
    "print(\"HIMYM dataset loaded with columns:\", himym_data.columns.tolist())\n",
    "print(\"TBBT dataset loaded with columns:\", tbbt_data.columns.tolist())\n",
    "print(\"Friends dataset loaded with columns:\", friends_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Individual Series\n",
    "Apply the function to process each dataset with its specific parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_mapping = {\n",
    "    'HIMYM': {'Ted': 'Male', 'Marshall': 'Male', 'Barney': 'Male', 'Lily': 'Female', 'Robin': 'Female'},\n",
    "    'Friends': {'Ross': 'Male', 'Monica': 'Female', 'Chandler': 'Male', 'Rachel': 'Female', 'Joey': 'Male', 'Phoebe': 'Female'},\n",
    "    'TBBT': {'Sheldon': 'Male', 'Leonard': 'Male', 'Penny': 'Female', 'Howard': 'Male', 'Raj': 'Male', 'Amy': 'Female', 'Bernadette': 'Female'}\n",
    "}\n",
    "\n",
    "\n",
    "himym_data_unified = unify_columns(himym_data, 'HIMYM', 'name', 'line')\n",
    "tbbt_data_unified = unify_columns(tbbt_data, 'TBBT', 'person_scene', 'dialogue')\n",
    "friends_data_unified = unify_columns(friends_data, 'Friends', 'character', 'line', capitalize_characters=True)\n",
    "\n",
    "\n",
    "tbbt_data_processed = process_dataset(\n",
    "    tbbt_data_unified, \n",
    "    'TBBT', \n",
    "    'character', \n",
    "    ['bazinga', 'spock', 'sweetie', 'honey', 'mother', 'jewish', 'science', 'nerd', 'knock']\n",
    ")\n",
    "\n",
    "himym_data_processed = process_dataset(\n",
    "    himym_data_unified, \n",
    "    'HIMYM', \n",
    "    'character', \n",
    "    ['suit up', 'legend', 'met Ted', 'awesome', 'challenge accepted', 'lawyered', 'pause']\n",
    ")\n",
    "\n",
    "friends_data_processed = process_dataset(\n",
    "    friends_data_unified, \n",
    "    'Friends', \n",
    "    'character', \n",
    "    ['how you doin', 'be more', 'on a break', 'oh my god', 'smelly cat', 'i know']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine All Processed Datasets\n",
    "The final step combines all processed datasets (`HIMYM`, `Friends`, and `TBBT`) into one comprehensive dataset. The `series` column differentiates between the shows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All datasets have been successfully combined and saved as 'FINAL_combined_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "combined_data = pd.concat([himym_data_processed, friends_data_processed, tbbt_data_processed], ignore_index=True)\n",
    "\n",
    "\n",
    "combined_data.to_csv('FINAL_combined_data.csv', index=False)\n",
    "\n",
    "print(\"All datasets have been successfully combined and saved as 'FINAL_combined_data.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
